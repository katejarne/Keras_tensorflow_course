{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kathy/.virtualenvs/keras_py_3_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py:1752: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "## To work well with gpu memory\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "#You should  import these packages and  the MNIST dataset\n",
    "\n",
    "import numpy as np\n",
    "import mnist\n",
    "import keras\n",
    "\n",
    "# The first time you run this might be a bit slow, since the\n",
    "# mnist package has to download and cache the data.\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "print(train_images.shape) # (60000, 28, 28)\n",
    "print(train_labels.shape) # (60000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Before we begin, we’ll normalize the image pixel values from [0, 255] to [-0.5, 0.5] \n",
    "to make our network easier to train (using smaller, centered values usually leads to better results). \n",
    "We’ll also reshape each image from (28, 28) to (28, 28, 1)\n",
    "because Keras requires the third dimension.\n",
    "'''\n",
    "\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "print(train_images.shape) # (60000, 28, 28, 1)\n",
    "print(test_images.shape)  # (10000, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- num_filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.\\n- The first layer in any Sequential model must specify the input_shape, so we do so on Conv2D. \\n- Once this input shape is specified, Keras will automatically infer the shapes of inputs for later layers.\\n- The output Softmax layer has 10 nodes, one for each class.\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Every Keras model is either built using the Sequential class,\n",
    "which represents a linear stack of layers, or the functional Model class, which is more customizeable.\n",
    "We’ll be using the simpler Sequential model, since our CNN will be a linear stack of layers.\n",
    "We start by instantiating a Sequential model:\n",
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "'''\n",
    "- num_filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.\n",
    "- The first layer in any Sequential model must specify the input_shape, so we do so on Conv2D. \n",
    "- Once this input shape is specified, Keras will automatically infer the shapes of inputs for later layers.\n",
    "- The output Softmax layer has 10 nodes, one for each class.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Compiling the Model\n",
    "Before we can begin training, we need to configure the training process. \n",
    "We decide 3 key factors during the compilation step:\n",
    "\n",
    "The optimizer. We’ll stick with a pretty good default: the Adam gradient-based optimizer. \n",
    "Keras has many other optimizers you can look into as well.\n",
    "The loss function. Since we’re using a Softmax output layer, we’ll use the Cross-Entropy loss.\n",
    "Keras distinguishes between binary_crossentropy (2 classes) and categorical_crossentropy (>2 classes), \n",
    "so we’ll use the latter. See all Keras losses.\n",
    "A list of metrics. Since this is a classification problem, we’ll just have Keras report on the accuracy metric.\n",
    "Here’s what that compilation looks like:\n",
    "\n",
    "'''\n",
    "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.3637 - accuracy: 0.8954 - val_loss: 0.2192 - val_accuracy: 0.9388\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.1850 - accuracy: 0.9478 - val_loss: 0.1477 - val_accuracy: 0.9554\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.1325 - accuracy: 0.9623 - val_loss: 0.1185 - val_accuracy: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe9611baf60>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Training a model in Keras literally consists only of calling fit() and specifying some parameters. \n",
    "There are a lot of possible parameters, but we’ll only supply these:\n",
    "\n",
    "The training data (images and labels), commonly known as X and Y, respectively.\n",
    "The number of epochs (iterations over the entire dataset) to train for.\n",
    "The validation data (or test data), which is used during training to periodically measure \n",
    "the network’s performance against data it hasn’t seen before.\n",
    "\n",
    "There’s one thing we have to be careful about:\n",
    "Keras expects the training targets to be 10-dimensional vectors, \n",
    "since there are 10 nodes in our Softmax output layer. Right now,\n",
    "our train_labels and test_labels arrays contain single integers representing the class for each image:\n",
    "'''\n",
    "train_labels = mnist.train_labels()\n",
    "print(train_labels[0]) # 5\n",
    "\n",
    "'''\n",
    "import mnist\n",
    "\n",
    "train_labels = mnist.train_labels()\n",
    "print(train_labels[0]) # 5\n",
    "Conveniently, Keras has a utility method that fixes this exact issue: to_categorical. \n",
    "It turns our array of class integers into an array of one-hot vectors instead.\n",
    "For example, 2 would become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] (it’s zero-indexed).\n",
    "\n",
    "Here’s what that looks like:\n",
    "\n",
    "'''\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model.fit(  train_images,  to_categorical(train_labels),  epochs=3, \n",
    "          validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using the trained model to make predictions is easy: we pass an array of inputs to predict() \n",
    "and it returns an array of outputs. Keep in mind that the output of our network is 10 probabilities \n",
    "(because of softmax),\n",
    "so we’ll use np.argmax() to turn those into actual digits.\n",
    "'''\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "print(test_images[0].shape)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(test_images[i].reshape(28, 28),cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
